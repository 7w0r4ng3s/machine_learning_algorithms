<!DOCTYPE html>
<html>
	<head>
		<meta name="viewport" content="width=device-width, initial-scale=1.0">
		<meta charset="utf-8" />
		<link rel="stylesheet" type="text/css" href="css/style.css" />
		<title>Naive Bayes</title>
	</head>
<body>
<h2>Naive Bayes</h2>

<p>A set of supervised learning algorithms based on applying Bayes theorem with the ‘naive’ assumption of independence between every pair of features.</p>

<figure><img src="Screen%20Shot%202018-09-13%20at%205.32.42%20PM.jpg"/></figure>

<ul>
	<li>Works well in document classification and spam filtering</li>
	<li>Require a small amount of training data to estimate the necessary parameters</li>
	<li>Fast</li>
	<li>Good classifier, bad estimator</li>
</ul>

<h2>Gaussian Naive Bayes</h2>

<figure><img src="Screen%20Shot%202018-09-13%20at%205.37.58%20PM.jpg"/></figure>

<pre><code class="code-highlighted code-python"><span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> <span class="syntax-all syntax-keyword">from</span> sklearn <span class="syntax-all syntax-keyword">import</span> datasets
<span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> iris <span class="syntax-all syntax-keyword">=</span> datasets.load_iris()
<span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> <span class="syntax-all syntax-keyword">from</span> sklearn.naive_bayes <span class="syntax-all syntax-keyword">import</span> GaussianNB
<span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> gnb <span class="syntax-all syntax-keyword">=</span> GaussianNB()
<span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> y_pred <span class="syntax-all syntax-keyword">=</span> gnb.fit(iris.data, iris.target).predict(iris.data)
<span class="syntax-all syntax-keyword">&gt;&gt;&gt;</span> <span class="syntax-all syntax-constant">print</span>(<span class="syntax-all syntax-string">&quot;Number of mislabeled points out of a total </span><span class="syntax-all syntax-constant">%d</span><span class="syntax-all syntax-string"> points : </span><span class="syntax-all syntax-error">
</span><span class="syntax-all syntax-constant">...</span>		<span class="syntax-all syntax-keyword">%</span>d<span class="syntax-all syntax-string">&quot; % (iris.data.shape[0],(iris.target != y_pred).sum()))</span><span class="syntax-all syntax-error">
</span>Number of mislabeled points out of a total <span class="syntax-all syntax-constant">150</span> points : <span class="syntax-all syntax-constant">6</span></code></pre>

<h2>Multinomial Naive Bayes</h2>

<p><code>MultinomialNB</code> implements the naive Bayes algorithm for multinomially distributed data.</p>

<ul>
	<li>Used in text classification</li>
	<li>Used for discrete counts</li>
</ul>

<h2>Bernoulli Naive Bayes</h2>

<p><code>BernoulliNM</code> implements the naive Bayes training and classification algorithms for data that is distributed according to multivairate Bernoulli distributions</p>

<ul>
	<li>May be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable</li>
	<li>Requires samples to be represented as binary-valued feature vectors</li>
</ul>

<figure><img src="Screen%20Shot%202018-09-13%20at%205.48.08%20PM.jpg"/></figure>

<p><strong>Differs from multinomial NB’s rule</strong>:</p>

<ul>
	<li>Explicitly penalizes the non-occurrence of a feature <em>i</em> that is an indicator for class <em>y</em></li>
	<li>Multinomial variant would simply ignore a non-occurring feature</li>
	<li>Perform better on datasets with shorter documents</li>
	<li>Evaluate both models if time permits</li>
</ul>

<h2>Out-of-core Naive Bayes Model Fitting</h2>

<p><code>MultinomialNB</code>, <code>BernoulliNB</code>, and <code>GaussianNB</code> expose a <code>partial_fit</code> method that can be used incrementally as done with other classifiers.</p>

<p>All naive Bayes classifiers support <em>sample weighting</em>.</p>

<p>Contrary to the <code>fit</code> method, the first call to <code>partial_fit</code> needs to be passed the list of all the expected class labels.</p>

<h2>Calculating Accuracy Score</h2>

<pre><code class="code-highlighted code-python"><span class="syntax-all syntax-keyword">def</span> <span class="syntax-all syntax-entity">NBAccuracy</span>(<span class="syntax-all syntax-parameter">features_train</span>, <span class="syntax-all syntax-parameter">labels_train</span>, <span class="syntax-all syntax-parameter">features_test</span>, <span class="syntax-all syntax-parameter">labels_test</span>):
	<span class="syntax-all syntax-keyword">from</span> sklearn.naive_bayes <span class="syntax-all syntax-keyword">import</span> GaussianNB
	<span class="syntax-all syntax-keyword">from</span> sklearn.metrics <span class="syntax-all syntax-keyword">import</span> accuracy_score

	clf <span class="syntax-all syntax-keyword">=</span> GaussianNB()
	clf.fit(features_train, labels_train)

	pred <span class="syntax-all syntax-keyword">=</span> clf.predict(features_test)

	accuracy <span class="syntax-all syntax-keyword">=</span> accuracy_score(labels_test, pred)
	<span class="syntax-all syntax-keyword">return</span> accuracy</code></pre>

<h2><a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Rule</a> (also called Bayes Theorem)</h2>

<figure><img src="Screen%20Shot%202018-09-13%20at%207.17.31%20PM.jpg"/></figure>

<h2>Why Naive Bayes naive?</h2>

<p>Naive bayes ignores words order, and it just cares about word frequency.</p>

</body>
</html>

